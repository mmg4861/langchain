{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70bec58",
   "metadata": {},
   "source": [
    "# ✂️ 텍스트 분할(Text Splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e41bd",
   "metadata": {},
   "source": [
    "### 📄 문서 분할\n",
    "- 로드된 문서들을 효율적으로 처리하기 위한 시스템\n",
    "\n",
    "- 목적 : 나중에 사용자가 입력한 질문에 대하여 효율적인 정보만 선별하여 가져오기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4777b0",
   "metadata": {},
   "source": [
    "### ☑️ 분할 필요성\n",
    "1. 정확성 : 질문에 연광성있는 정보만 제공\n",
    "\n",
    "2. 효율성 : 리소스를 최적화, 비용 문제, 할루네이션 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5145b",
   "metadata": {},
   "source": [
    "### ☑️ 분할 과정\n",
    "1. 문서 구조 파악\n",
    "\n",
    "2. 단위 선정\n",
    "\n",
    "3. 단위 크기 선정\n",
    "\n",
    "4. 청크 오버랩\n",
    "\n",
    "`-> PDF, 웹페이지 등의 문서 구조를 파악하고 어떤 단위로 나눌 지 결정하고 몇개의 토큰으로 나눌 지 결정합니다.`\n",
    "\n",
    "`-> 분할된 끝 부분에서 맥락이 이어질 수 있도록 오버랩합니다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f600f",
   "metadata": {},
   "source": [
    "### ☑️ Chunk 분할 시각화\n",
    "\n",
    "- 링크 : https://chunkviz.up.railway.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f604d8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1722db5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76dc408",
   "metadata": {},
   "source": [
    "## 📌 문자 텍스트 분할(CharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dc299",
   "metadata": {},
   "source": [
    "### 💬 문자 기반 텍스트 분할이란?\n",
    "\n",
    "긴 텍스트를 문자 수 기준으로 잘라서 `조각(chunk)`으로 만드는 작업\n",
    "\n",
    "예를 들어, 한번에 LLM에 넣기에 긴 문 서가 있을 경우 잘게 쪼개어 처리한다.                                   -> LLM 토큰 제한, 성능 하락"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0491d2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a27e3",
   "metadata": {},
   "source": [
    "### 텍스트 분할 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932c96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2f3df",
   "metadata": {},
   "source": [
    "### 문자 텍스트 분할기 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb4af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbad1a0",
   "metadata": {},
   "source": [
    "### 실습 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac51749",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다.\n",
    "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다.\n",
    "LLM에서 효율적인 역할을 할 수 있습니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2bbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ae7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f77f2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조각 1:\n",
      "LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다\n",
      "\n",
      "조각 2:\n",
      "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다\n",
      "\n",
      "조각 3:\n",
      "LLM에서 효율적인 역할을 할 수 있습니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"조각 {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91766e8b",
   "metadata": {},
   "source": [
    "### 짚어보기\n",
    "- `chunk_size` = 한 조각의 최대 문자 길이\n",
    "\n",
    "- `chunk_overlap` = 조각 간 겹치는 부분을 자연스럽게 이어주기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebf721",
   "metadata": {},
   "source": [
    "### 더 깊숙하게 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0361fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    \n",
    "    separator=\" \",\n",
    "    \n",
    "    chunk_size=250,\n",
    "    \n",
    "    chunk_overlap=50,\n",
    "    \n",
    "    length_function=len,\n",
    "    \n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b3918",
   "metadata": {},
   "source": [
    "`separator` = 분할 기준 설정이며 기본 값은 \"\\n\\n\" 입니다.\n",
    "\n",
    "`chunk_size` = 청크 최대 크기 설정\n",
    "\n",
    "`chunk_overlap` = 인접한 청크 간의 허용되는 중복 문자 수 지정\n",
    "\n",
    "`length_function` = 텍스트의 길이를 계산하는 함수 지정\n",
    "\n",
    "`is_separator_regex` = 구분자를 정규식인지 아닌지 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96bf84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3176ee4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca07da",
   "metadata": {},
   "source": [
    "## 📌 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def39c9",
   "metadata": {},
   "source": [
    "### ⏎ 재귀적 문자 텍스트 분할이란?\n",
    "\n",
    "- 어떤 규칙으로 자르다가 안되면 다음 규칙으로 다시 시도한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b9630",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65334bd2",
   "metadata": {},
   "source": [
    "### 재귀적 방법 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ef2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcb578",
   "metadata": {},
   "source": [
    "### 기본 문자 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67727f6",
   "metadata": {},
   "source": [
    "- 청크가 충분히 작아질 때까지 주어진 문자 목록의 [ 단락 -> 문장 -> 단어 ] 순서대로 분할합니다.\n",
    "\n",
    "- 커스텀하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308a178",
   "metadata": {},
   "source": [
    "### 실습 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba3c44",
   "metadata": {},
   "source": [
    "- 재귀적 문자 텍스트 분할 디폴트값 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5269c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조각 1:\n",
      "LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다.\n",
      "\n",
      "조각 2:\n",
      "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다.\n",
      "\n",
      "조각 3:\n",
      "LLM에서 효율적인 역할을 할 수 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 재귀적 문자 텍스트 분할기 기본값 사용\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다.\n",
    "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다.\n",
    "LLM에서 효율적인 역할을 할 수 있습니다.\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"조각 {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed4cc3",
   "metadata": {},
   "source": [
    "- 커스터 마이징해서 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e61f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조각 1:\n",
      "LangChain은 다양한 LLM 응용을 쉽게 만들 수\n",
      "\n",
      "조각 2:\n",
      "쉽게 만들 수 있는 프레임워크입니다\n",
      "\n",
      "조각 3:\n",
      "!\n",
      "텍스트 분할은 긴 문서를 작은 조각으로 나누는\n",
      "\n",
      "조각 4:\n",
      "조각으로 나누는 작업입니다\n",
      "\n",
      "조각 5:\n",
      "?\n",
      "LLM에서 효율적인 역할을 할 수 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 커스터 마이징\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다!\n",
    "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다?\n",
    "LLM에서 효율적인 역할을 할 수 있습니다.\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"?\", \"!\", \" \", \"\"],\n",
    "    chunk_size=30,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"조각 {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1607ecb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87919b8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c40eeb8",
   "metadata": {},
   "source": [
    "## 📌 토큰 텍스트 분할(TokenTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b6330",
   "metadata": {},
   "source": [
    "### 🪙 토큰 기반 텍스트 분할이란?\n",
    "\n",
    "- 텍스트를 문자 수가 아니라 토큰 개수 기준으로 분할하는 방식\n",
    "\n",
    "- 언어 모델에 존재하는 토큰 제한에 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1056c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af76c31",
   "metadata": {},
   "source": [
    "### TokenTextSpliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9949d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조각 1:\n",
      "LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프�\n",
      "\n",
      "조각 2:\n",
      "�는 프레임워크입니다.\n",
      "텍스트 분할은 긴 �\n",
      "\n",
      "조각 3:\n",
      "�할은 긴 문서를 작은 조각으로 나누는 작업\n",
      "\n",
      "조각 4:\n",
      "�는 작업입니다.\n",
      "LLM에서 효율적인 역할을 할 �\n",
      "\n",
      "조각 5:\n",
      "��할을 할 수 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text = \"\"\"LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다.\n",
    "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다.\n",
    "LLM에서 효율적인 역할을 할 수 있습니다.\"\"\"\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"조각 {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2fbe4",
   "metadata": {},
   "source": [
    "### ☑️ 토큰이란?\n",
    "\n",
    "- 토큰은 LLM이 실제로 이해하고 처리하는 단어 단위보다 작은 텍스트 조각\n",
    "\n",
    "- 이전 문자 텍스트 분할은 chunk 사이즈가 문자 수 기준이므로 50자 이내를 안넘어서 줄바뀜마다 조각낼 수 있었으며, 토큰은 그보다 작은 단위로써 더 많이 조각나게 된다.\n",
    "\n",
    "- 인코딩 깨짐 현상은 토큰 단위로 자르기 때문에 토크나이저가 한글을 토큰 단위로 정확히 끊지 못하는 현상이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967f0d8",
   "metadata": {},
   "source": [
    "### tiktoken\n",
    "\n",
    "- tiktoken은 OpenAI에서 만든 빠른 BPE Tokenizer 입니다.\n",
    "\n",
    "- TokenTextSplitter에 내장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934acc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-text-splitters tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a9762",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "- 자연어 처리를 위한 라이브러리\n",
    "\n",
    "- Python, Cyhon으로 작성된 라이브러리\n",
    "\n",
    "- chunk_size는 문자 수로 측정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61829bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-core 0.11.23 requires numpy<2.0.0, but you have numpy 2.2.4 which is incompatible.\n",
      "langchain-chroma 0.2.2 requires numpy<2.0.0,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.4 which is incompatible.\n",
      "langchain-azure-ai 0.1.2 requires numpy<2.0,>=1.24, but you have numpy 2.2.4 which is incompatible.\n",
      "unstructured 0.17.0 requires numpy<2, but you have numpy 2.2.4 which is incompatible.\n",
      "pinecone-text 0.10.0 requires numpy<2.0,>=1.21.5; python_version < \"3.12\", but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30d0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38395d2",
   "metadata": {},
   "source": [
    "- 자연어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc208f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text = \"\"\"LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다.\n",
    "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다.\n",
    "LLM에서 효율적인 역할을 할 수 있습니다.\"\"\"\n",
    "\n",
    "splitter = SpacyTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    ")\n",
    "\n",
    "text = splitter.split_text(text)\n",
    "\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd2ffa",
   "metadata": {},
   "source": [
    "- 비자연어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71339f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/project/main.py\n",
      "<div class='item'>Hello</div>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text = \"\"\"/home/user/project/main.py\n",
    "<div class='item'>Hello</div>\"\"\"\n",
    "\n",
    "splitter = SpacyTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "text = splitter.split_text(text)\n",
    "\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff43f91",
   "metadata": {},
   "source": [
    "### SentenceTransformers\n",
    "\n",
    "- 문장, 문단, 문서 등을 숫자 벡터로 변환해주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dfc03e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408bb03545d943a889ba444dd4bffc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8d0659425f485dbbbfcea32d5659c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb44bbd9b31944d9a99fb4c52cde0ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1d45964bf6489ebc5a0c3c1969043a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b8c5305abe47728f741595930bc38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55690a35f60845e2acc9f0a5c4c6bfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45ab741306746aba033464a5e7ce6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f858dd632a42f3974eb96c8622bc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b21f2845e04129a22dd0c1648645ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c714621c51143c8b669a58e0803b759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5383ca3744e347508698045d8a2fbb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "text = \"\"\"LangChain은 다양한 LLM 응용을 쉽게 만들 수 있는 프레임워크입니다.\n",
    "텍스트 분할은 긴 문서를 작은 조각으로 나누는 작업입니다.\n",
    "LLM에서 효율적인 역할을 할 수 있습니다.\"\"\"\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_size=50, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "count_start_and_stop_tokens = 2\n",
    "\n",
    "# 텍스트의 토큰 개수에서 시작과 종료 토큰의 개수를 뺍니다.\n",
    "text_token_count = splitter.count_tokens(\n",
    "    text=text) - count_start_and_stop_tokens\n",
    "print(text_token_count)  # 계산된 텍스트 토큰 개수를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f02ce",
   "metadata": {},
   "source": [
    "### ☑️ BERT 계열 토크나이저\n",
    "\n",
    "- [CLS] : 문장의 시작을 나타내는 스페셜 토큰\n",
    "- [SEP] : 문장 끝을 나타내는 스페셜 토큰\n",
    "\n",
    "총 토큰 수 = 본문 토큰 수 + 2 가 되므로 처음과 끝의 토큰 수를 빼고 계산해서 본문 토큰 개수를 출력하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b6a73",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "- Natural Language Toolkit\n",
    "\n",
    "- 자연어 처리를 위한 라이브러리와 프로그램 모음\n",
    "\n",
    "- \\n\\n 방식이 아닌 NLTK 방식으로 분할할 때 사용합니다.\n",
    "\n",
    "- 전처리, 토큰화, 형태소 분석, 품사 태깅 등 수행 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32477376",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "\n",
    "- 한국어 텍스트 전처리\n",
    "\n",
    "- 감정 분석\n",
    "\n",
    "- 문장 단위 처리\n",
    "\n",
    "- 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd2cb6",
   "metadata": {},
   "source": [
    "### Hugging Face tokenizer\n",
    "\n",
    "- Huggin Face 토크나이저에 의해 계산된 토큰 수를 기준으로 분할\n",
    "\n",
    "- 아래의 방법으로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# GPT-2 모델의 토크나이저를 불러옵니다.\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9293287",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76beecfe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b2529",
   "metadata": {},
   "source": [
    "## 📌 시멘틱 청커(SemanticChunker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717cc8c8",
   "metadata": {},
   "source": [
    "### 💡 시멘틱 청커란 ?\n",
    "\n",
    "- 의미 기반으로 텍스트를 분할하는 것이다.\n",
    "\n",
    "- 따라서 뚝뚝 끊김 없이 자연스러운 흐름을 유지할 수 있다.\n",
    "\n",
    "- 임베딩 모델에 주로 쓰인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5121c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfa3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 0.3.23\n",
      "Uninstalling langchain-0.3.23:\n",
      "  Successfully uninstalled langchain-0.3.23\n",
      "Found existing installation: langchain-openai 0.3.13\n",
      "Uninstalling langchain-openai-0.3.13:\n",
      "  Successfully uninstalled langchain-openai-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.52)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.16)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (1.74.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: langchain-openai, langchain\n",
      "Successfully installed langchain-0.3.23 langchain-openai-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall langchain langchain-openai -y\n",
    "%pip install --upgrade --no-cache-dir langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73799be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a224c3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bf99f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a6659",
   "metadata": {},
   "source": [
    "## 📌 코드 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43682991",
   "metadata": {},
   "source": [
    "### 🧑‍💻 Split Code\n",
    "\n",
    "- 다양한 프로그래밍 언어를 분할할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fc393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c05f18",
   "metadata": {},
   "source": [
    "### 프로그래밍 언어별 분할 기준값을 확인하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4456fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (Language, RecursiveCharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8adca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl',\n",
       " 'haskell',\n",
       " 'elixir',\n",
       " 'powershell']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c053122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6c8df",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce4d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='hello_world()')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=50, \n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f071ec",
   "metadata": {},
   "source": [
    "- Document 생성하여 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b76958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='hello_world()')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = splitter.create_documents([PYTHON_CODE])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642973f4",
   "metadata": {},
   "source": [
    "- 필요한 분할 부분에서 구분자 표시하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f94dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "==================\n",
      "hello_world()\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for doc in result:\n",
    "    print(doc.page_content, end=\"\\n==================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce994f",
   "metadata": {},
   "source": [
    "### JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d7bab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "JS_CODE = \"\"\"\n",
    "function helloWorld() {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, \n",
    "    chunk_size=65, \n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25fbd565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'),\n",
       " Document(metadata={}, page_content='helloWorld();')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = splitter.create_documents([JS_CODE])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "964e03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function helloWorld() {\n",
      "  console.log(\"Hello, World!\");\n",
      "}\n",
      "==================\n",
      "helloWorld();\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for doc in result:\n",
    "    print(doc.page_content, end=\"\\n==================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c50d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7c2a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d580c2d",
   "metadata": {},
   "source": [
    "## 📌 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe1660",
   "metadata": {},
   "source": [
    "### Ⓜ️ 마크다운 헤더란?\n",
    "\n",
    "- 텍스트 기반의 간단한 문서 작성 언어이다.\n",
    "\n",
    "- 헤더는 문서의 제목이나 섹션을 구분한다.\n",
    "\n",
    "- 헤더는 # 기호로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd9a28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0696b2",
   "metadata": {},
   "source": [
    "### Markdown 문서의 헤더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e697e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 제목\n",
      "\n",
      "## 1. 소제목\n",
      "\n",
      "나는 문기다.\n",
      "\n",
      "그는 용이다.\n",
      "\n",
      "### 1-1. 부제목 \n",
      "\n",
      "그는 현이다. \n",
      "\n",
      "## 2. 제목\n",
      "\n",
      "그는 바보다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# 마크다운 형식의 문서를 문자열로 정의합니다.\n",
    "markdown = \"# 제목\\n\\n## 1. 소제목\\n\\n나는 문기다.\\n\\n그는 용이다.\\n\\n### 1-1. 부제목 \\n\\n그는 현이다. \\n\\n## 2. 제목\\n\\n그는 바보다.\"\n",
    "print(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1e96eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 문기다.  \n",
      "그는 용이다.\n",
      "{'Header 1': '제목', 'Header 2': '1. 소제목'}\n",
      "=====================\n",
      "그는 현이다.\n",
      "{'Header 1': '제목', 'Header 2': '1. 소제목', 'Header 3': '1-1. 부제목'}\n",
      "=====================\n",
      "그는 바보다.\n",
      "{'Header 1': '제목', 'Header 2': '2. 제목'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "headers_split = [\n",
    "    (\n",
    "        \"#\",\n",
    "        \"Header 1\",\n",
    "    ), \n",
    "    (\n",
    "        \"##\",\n",
    "        \"Header 2\",\n",
    "    ), \n",
    "    (\n",
    "        \"###\",\n",
    "        \"Header 3\",\n",
    "    ), \n",
    "]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_split)\n",
    "\n",
    "result = splitter.split_text(markdown)\n",
    "\n",
    "for header in result:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbfeb1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc8002",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a64e3",
   "metadata": {},
   "source": [
    "## 📌 HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590a07e",
   "metadata": {},
   "source": [
    "### 💾 HTML 헤더란?\n",
    "\n",
    "- 헤더 태그 : `<h1>, <h2>, <h3>, <h4>`\n",
    "\n",
    "- 제목 수준을 나타내는 태그값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e21a3",
   "metadata": {},
   "source": [
    "### HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "074663dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n",
      "Some intro text about Foo.\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n",
      "Bar main section\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section'}\n",
      "=====================\n",
      "Some intro text about Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section'}\n",
      "=====================\n",
      "Bar subsection 1\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}\n",
      "=====================\n",
      "Some text about the first subtopic of Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}\n",
      "=====================\n",
      "Bar subsection 2\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}\n",
      "=====================\n",
      "Some text about the second subtopic of Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}\n",
      "=====================\n",
      "Baz\n",
      "{'Header 1': 'Foo', 'Header 2': 'Baz'}\n",
      "=====================\n",
      "Some text about Baz  \n",
      "Some concluding text about Foo\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_split = [\n",
    "    (\"h1\", \"Header 1\"),  # 분할할 헤더 태그와 해당 헤더의 이름을 지정합니다.\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_split)\n",
    "\n",
    "result = splitter.split_text(html_string)\n",
    "\n",
    "for header in result:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b16d6b",
   "metadata": {},
   "source": [
    "### URL에서 HTML 로드 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c13b7fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Theorem 2 there is a sentence φ with the property  \n",
      "y  \n",
      "x  \n",
      "x  \n",
      "y  \n",
      "[ ]  \n",
      "12  \n",
      "⊢ (φ ↔\n",
      "¬Prov( )).  \n",
      "P  \n",
      "⌈  \n",
      "φ  \n",
      "⌉  \n",
      "Thus φ says ‘I am not provable.’ We now observe, if ⊢ φ, then by (1) there is such that ⊢ Prf( , ), hence ⊢ Prov( ), hence,\n",
      "by (3) ⊢ ¬φ, so is inconsistent.\n",
      "Thus  \n",
      "P  \n",
      "n  \n",
      "P  \n",
      "n  \n",
      "⌈  \n",
      "φ  \n",
      "⌉  \n",
      "P  \n",
      "⌈  \n",
      "φ  \n",
      "⌉  \n",
      "P  \n",
      "P  \n",
      "⊬ φ  \n",
      "P  \n",
      "Furthermore, by (4) and (2), we have ⊢\n",
      "¬Prf( , ) for all natural\n",
      "numbers . By ω-consistency ⊬\n",
      "∃ Prf( , ). Thus (3) gives ⊬ ¬φ. We have shown that if is\n",
      "{'Header 1': 'Kurt Gödel'}\n",
      "=====================\n",
      "ω-consistent, then φ is independent of .  \n",
      "P  \n",
      "n  \n",
      "⌈  \n",
      "φ  \n",
      "⌉  \n",
      "n  \n",
      "P  \n",
      "x  \n",
      "x  \n",
      "⌈  \n",
      "φ  \n",
      "⌉  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "On concluding the proof of the first theorem, Gödel remarks,\n",
      "“we can readily see that the proof just given is constructive;\n",
      "that is … proved in an intuitionistically unobjectionable\n",
      "manner…” (Gödel 1986, p. 177). This is because, as\n",
      "he points out, all the existential statements are based on his theorem\n",
      "V (giving the numeralwise expressibility of primitive recursive\n",
      "{'Header 1': 'Kurt Gödel'}\n",
      "=====================\n",
      "relations), which is intuitionistically unobjectionable.  \n",
      "2.2.3 The Second Incompleteness Theorem  \n",
      "The Second Incompleteness Theorem establishes the unprovability, in\n",
      "number theory, of the consistency of number theory. First we have to\n",
      "write down a number-theoretic formula that expresses the consistency\n",
      "of the axioms. This is surprisingly simple. We just let\n",
      "Con( ) be the sentence ¬Prov( ).  \n",
      "P  \n",
      "⌈  \n",
      "0 =\n",
      "1  \n",
      "⌉  \n",
      "(Gödel’s Second Incompleteness\n",
      "Theorem) If is consistent, then Con( ) is not\n",
      "{'Header 1': 'Kurt Gödel'}\n",
      "=====================\n",
      "provable from .  \n",
      "Theorem 4  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "Let φ be as in (3). The reasoning used to infer\n",
      "‘if ⊢ φ, then ⊢ 0 ≠\n",
      "1‘ does not go beyond elementary number theory, and can\n",
      "therefore, albeit with a lot of effort (see below), be formalized in . This yields: ⊢\n",
      "(Prov( ) →\n",
      "¬Con( )), and thus by (3), ⊢\n",
      "(Con( ) → φ). Since ⊬ φ, we\n",
      "must have ⊬ Con( ).  \n",
      "Proof:  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "⌈  \n",
      "φ  \n",
      "⌉  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "The above proof (sketch) of the Second Incompleteness Theorem is\n",
      "{'Header 1': 'Kurt Gödel'}\n",
      "=====================\n",
      "deceptively simple as it avoids the formalization. A rigorous proof\n",
      "would have to establish the proof of ‘if ⊢\n",
      "φ, then ⊢ 0 ≠ 1’ in .  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "It is noteworthy that ω-consistency is not needed in the proof\n",
      "of Gödel’s Second Incompleteness Theorem. Also note that\n",
      "neither is ¬Con( ) provable, by the consistency of and the fact, now known as Löb’s theorem, that ⊢\n",
      "Prov( ) implies ⊢ φ.  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "⌈  \n",
      "φ  \n",
      "⌉  \n",
      "P  \n",
      "The assumption of ω-consistency in the First Incompleteness\n",
      "{'Header 1': 'Kurt Gödel'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "\n",
    "headers_split = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_split)\n",
    "\n",
    "html_header_splits = splitter.split_text_from_url(url)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "\n",
    "# 분할된 텍스트 중 80번째부터 85번째까지의 청크를 출력합니다.\n",
    "for header in splits[80:85]:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644760d",
   "metadata": {},
   "source": [
    "- 특정 헤더를 누락시키며, 구조적 차이에 대한 한계가 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3242b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403a76c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e5265",
   "metadata": {},
   "source": [
    "## 📌 재귀적 JSON 분할(RecursiveJsonSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff13600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# JSON 데이터를 로드합니다.\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd4c35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63edd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_chunks = splitter.split_json(json_data=json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6dbcf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}\n",
      "=======================================================================================================================================================================================================================================\n",
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}\n"
     ]
    }
   ],
   "source": [
    "# JSON 데이터를 문서 생성\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "# JSON 데이터를 문자열 청크 생성\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "# 첫 번째 문자열을 출력\n",
    "print(docs[0].page_content)\n",
    "\n",
    "print(\"===\" * 77)\n",
    "\n",
    "# 분할된 문자열 청크를 출력\n",
    "print(texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
