{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70bec58",
   "metadata": {},
   "source": [
    "# âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e41bd",
   "metadata": {},
   "source": [
    "### ğŸ“„ ë¬¸ì„œ ë¶„í• \n",
    "- ë¡œë“œëœ ë¬¸ì„œë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ\n",
    "\n",
    "- ëª©ì  : ë‚˜ì¤‘ì— ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ íš¨ìœ¨ì ì¸ ì •ë³´ë§Œ ì„ ë³„í•˜ì—¬ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4777b0",
   "metadata": {},
   "source": [
    "### â˜‘ï¸ ë¶„í•  í•„ìš”ì„±\n",
    "1. ì •í™•ì„± : ì§ˆë¬¸ì— ì—°ê´‘ì„±ìˆëŠ” ì •ë³´ë§Œ ì œê³µ\n",
    "\n",
    "2. íš¨ìœ¨ì„± : ë¦¬ì†ŒìŠ¤ë¥¼ ìµœì í™”, ë¹„ìš© ë¬¸ì œ, í• ë£¨ë„¤ì´ì…˜ ë°©ì§€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5145b",
   "metadata": {},
   "source": [
    "### â˜‘ï¸ ë¶„í•  ê³¼ì •\n",
    "1. ë¬¸ì„œ êµ¬ì¡° íŒŒì•…\n",
    "\n",
    "2. ë‹¨ìœ„ ì„ ì •\n",
    "\n",
    "3. ë‹¨ìœ„ í¬ê¸° ì„ ì •\n",
    "\n",
    "4. ì²­í¬ ì˜¤ë²„ë©\n",
    "\n",
    "`-> PDF, ì›¹í˜ì´ì§€ ë“±ì˜ ë¬¸ì„œ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³  ì–´ë–¤ ë‹¨ìœ„ë¡œ ë‚˜ëˆŒ ì§€ ê²°ì •í•˜ê³  ëª‡ê°œì˜ í† í°ìœ¼ë¡œ ë‚˜ëˆŒ ì§€ ê²°ì •í•©ë‹ˆë‹¤.`\n",
    "\n",
    "`-> ë¶„í• ëœ ë ë¶€ë¶„ì—ì„œ ë§¥ë½ì´ ì´ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ ì˜¤ë²„ë©í•©ë‹ˆë‹¤.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f600f",
   "metadata": {},
   "source": [
    "### â˜‘ï¸ Chunk ë¶„í•  ì‹œê°í™”\n",
    "\n",
    "- ë§í¬ : https://chunkviz.up.railway.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f604d8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1722db5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76dc408",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (CharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dc299",
   "metadata": {},
   "source": [
    "### ğŸ’¬ ë¬¸ì ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ì´ë€?\n",
    "\n",
    "ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ì„œ `ì¡°ê°(chunk)`ìœ¼ë¡œ ë§Œë“œëŠ” ì‘ì—…\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, í•œë²ˆì— LLMì— ë„£ê¸°ì— ê¸´ ë¬¸ ì„œê°€ ìˆì„ ê²½ìš° ì˜ê²Œ ìª¼ê°œì–´ ì²˜ë¦¬í•œë‹¤.                                   -> LLM í† í° ì œí•œ, ì„±ëŠ¥ í•˜ë½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0491d2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a27e3",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ë¶„í•  íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932c96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2f3df",
   "metadata": {},
   "source": [
    "### ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb4af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbad1a0",
   "metadata": {},
   "source": [
    "### ì‹¤ìŠµ ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac51749",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2bbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ae7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f77f2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¡°ê° 1:\n",
      "LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤\n",
      "\n",
      "ì¡°ê° 2:\n",
      "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤\n",
      "\n",
      "ì¡°ê° 3:\n",
      "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"ì¡°ê° {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91766e8b",
   "metadata": {},
   "source": [
    "### ì§šì–´ë³´ê¸°\n",
    "- `chunk_size` = í•œ ì¡°ê°ì˜ ìµœëŒ€ ë¬¸ì ê¸¸ì´\n",
    "\n",
    "- `chunk_overlap` = ì¡°ê° ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì£¼ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebf721",
   "metadata": {},
   "source": [
    "### ë” ê¹Šìˆ™í•˜ê²Œ ì•Œì•„ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0361fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    \n",
    "    separator=\" \",\n",
    "    \n",
    "    chunk_size=250,\n",
    "    \n",
    "    chunk_overlap=50,\n",
    "    \n",
    "    length_function=len,\n",
    "    \n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b3918",
   "metadata": {},
   "source": [
    "`separator` = ë¶„í•  ê¸°ì¤€ ì„¤ì •ì´ë©° ê¸°ë³¸ ê°’ì€ \"\\n\\n\" ì…ë‹ˆë‹¤.\n",
    "\n",
    "`chunk_size` = ì²­í¬ ìµœëŒ€ í¬ê¸° ì„¤ì •\n",
    "\n",
    "`chunk_overlap` = ì¸ì ‘í•œ ì²­í¬ ê°„ì˜ í—ˆìš©ë˜ëŠ” ì¤‘ë³µ ë¬¸ì ìˆ˜ ì§€ì •\n",
    "\n",
    "`length_function` = í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ ì§€ì •\n",
    "\n",
    "`is_separator_regex` = êµ¬ë¶„ìë¥¼ ì •ê·œì‹ì¸ì§€ ì•„ë‹Œì§€ ê²°ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96bf84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3176ee4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca07da",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (RecursiveCharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def39c9",
   "metadata": {},
   "source": [
    "### â ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• ì´ë€?\n",
    "\n",
    "- ì–´ë–¤ ê·œì¹™ìœ¼ë¡œ ìë¥´ë‹¤ê°€ ì•ˆë˜ë©´ ë‹¤ìŒ ê·œì¹™ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b9630",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65334bd2",
   "metadata": {},
   "source": [
    "### ì¬ê·€ì  ë°©ë²• ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ef2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcb578",
   "metadata": {},
   "source": [
    "### ê¸°ë³¸ ë¬¸ì ëª©ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67727f6",
   "metadata": {},
   "source": [
    "- ì²­í¬ê°€ ì¶©ë¶„íˆ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ ì£¼ì–´ì§„ ë¬¸ì ëª©ë¡ì˜ [ ë‹¨ë½ -> ë¬¸ì¥ -> ë‹¨ì–´ ] ìˆœì„œëŒ€ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì»¤ìŠ¤í…€í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308a178",
   "metadata": {},
   "source": [
    "### ì‹¤ìŠµ ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba3c44",
   "metadata": {},
   "source": [
    "- ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í•  ë””í´íŠ¸ê°’ ì´ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5269c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¡°ê° 1:\n",
      "LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì¡°ê° 2:\n",
      "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì¡°ê° 3:\n",
      "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"ì¡°ê° {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed4cc3",
   "metadata": {},
   "source": [
    "- ì»¤ìŠ¤í„° ë§ˆì´ì§•í•´ì„œ ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e61f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¡°ê° 1:\n",
      "LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜\n",
      "\n",
      "ì¡°ê° 2:\n",
      "ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤\n",
      "\n",
      "ì¡°ê° 3:\n",
      "!\n",
      "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ”\n",
      "\n",
      "ì¡°ê° 4:\n",
      "ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤\n",
      "\n",
      "ì¡°ê° 5:\n",
      "?\n",
      "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì»¤ìŠ¤í„° ë§ˆì´ì§•\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤!\n",
    "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤?\n",
    "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"?\", \"!\", \" \", \"\"],\n",
    "    chunk_size=30,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"ì¡°ê° {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1607ecb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87919b8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c40eeb8",
   "metadata": {},
   "source": [
    "## ğŸ“Œ í† í° í…ìŠ¤íŠ¸ ë¶„í• (TokenTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b6330",
   "metadata": {},
   "source": [
    "### ğŸª™ í† í° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ì´ë€?\n",
    "\n",
    "- í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì ìˆ˜ê°€ ì•„ë‹ˆë¼ í† í° ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ë°©ì‹\n",
    "\n",
    "- ì–¸ì–´ ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ” í† í° ì œí•œì— ìœ ìš©í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1056c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af76c31",
   "metadata": {},
   "source": [
    "### TokenTextSpliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9949d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¡°ê° 1:\n",
      "LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ï¿½\n",
      "\n",
      "ì¡°ê° 2:\n",
      "ï¿½ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ï¿½\n",
      "\n",
      "ì¡°ê° 3:\n",
      "ï¿½í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…\n",
      "\n",
      "ì¡°ê° 4:\n",
      "ï¿½ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
      "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ï¿½\n",
      "\n",
      "ì¡°ê° 5:\n",
      "ï¿½ï¿½í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text = \"\"\"LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"ì¡°ê° {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2fbe4",
   "metadata": {},
   "source": [
    "### â˜‘ï¸ í† í°ì´ë€?\n",
    "\n",
    "- í† í°ì€ LLMì´ ì‹¤ì œë¡œ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë‹¨ì–´ ë‹¨ìœ„ë³´ë‹¤ ì‘ì€ í…ìŠ¤íŠ¸ ì¡°ê°\n",
    "\n",
    "- ì´ì „ ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• ì€ chunk ì‚¬ì´ì¦ˆê°€ ë¬¸ì ìˆ˜ ê¸°ì¤€ì´ë¯€ë¡œ 50ì ì´ë‚´ë¥¼ ì•ˆë„˜ì–´ì„œ ì¤„ë°”ë€œë§ˆë‹¤ ì¡°ê°ë‚¼ ìˆ˜ ìˆì—ˆìœ¼ë©°, í† í°ì€ ê·¸ë³´ë‹¤ ì‘ì€ ë‹¨ìœ„ë¡œì¨ ë” ë§ì´ ì¡°ê°ë‚˜ê²Œ ëœë‹¤.\n",
    "\n",
    "- ì¸ì½”ë”© ê¹¨ì§ í˜„ìƒì€ í† í° ë‹¨ìœ„ë¡œ ìë¥´ê¸° ë•Œë¬¸ì— í† í¬ë‚˜ì´ì €ê°€ í•œê¸€ì„ í† í° ë‹¨ìœ„ë¡œ ì •í™•íˆ ëŠì§€ ëª»í•˜ëŠ” í˜„ìƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967f0d8",
   "metadata": {},
   "source": [
    "### tiktoken\n",
    "\n",
    "- tiktokenì€ OpenAIì—ì„œ ë§Œë“  ë¹ ë¥¸ BPE Tokenizer ì…ë‹ˆë‹¤.\n",
    "\n",
    "- TokenTextSplitterì— ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934acc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-text-splitters tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a9762",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "- ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "- Python, Cyhonìœ¼ë¡œ ì‘ì„±ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "- chunk_sizeëŠ” ë¬¸ì ìˆ˜ë¡œ ì¸¡ì •ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61829bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-core 0.11.23 requires numpy<2.0.0, but you have numpy 2.2.4 which is incompatible.\n",
      "langchain-chroma 0.2.2 requires numpy<2.0.0,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.4 which is incompatible.\n",
      "langchain-azure-ai 0.1.2 requires numpy<2.0,>=1.24, but you have numpy 2.2.4 which is incompatible.\n",
      "unstructured 0.17.0 requires numpy<2, but you have numpy 2.2.4 which is incompatible.\n",
      "pinecone-text 0.10.0 requires numpy<2.0,>=1.21.5; python_version < \"3.12\", but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30d0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38395d2",
   "metadata": {},
   "source": [
    "- ìì—°ì–´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc208f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text = \"\"\"LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "splitter = SpacyTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    ")\n",
    "\n",
    "text = splitter.split_text(text)\n",
    "\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd2ffa",
   "metadata": {},
   "source": [
    "- ë¹„ìì—°ì–´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71339f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/project/main.py\n",
      "<div class='item'>Hello</div>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text = \"\"\"/home/user/project/main.py\n",
    "<div class='item'>Hello</div>\"\"\"\n",
    "\n",
    "splitter = SpacyTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "text = splitter.split_text(text)\n",
    "\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff43f91",
   "metadata": {},
   "source": [
    "### SentenceTransformers\n",
    "\n",
    "- ë¬¸ì¥, ë¬¸ë‹¨, ë¬¸ì„œ ë“±ì„ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dfc03e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408bb03545d943a889ba444dd4bffc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8d0659425f485dbbbfcea32d5659c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb44bbd9b31944d9a99fb4c52cde0ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1d45964bf6489ebc5a0c3c1969043a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b8c5305abe47728f741595930bc38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55690a35f60845e2acc9f0a5c4c6bfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45ab741306746aba033464a5e7ce6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f858dd632a42f3974eb96c8622bc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b21f2845e04129a22dd0c1648645ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c714621c51143c8b669a58e0803b759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5383ca3744e347508698045d8a2fbb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "text = \"\"\"LangChainì€ ë‹¤ì–‘í•œ LLM ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ ë¶„í• ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "LLMì—ì„œ íš¨ìœ¨ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_size=50, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "count_start_and_stop_tokens = 2\n",
    "\n",
    "# í…ìŠ¤íŠ¸ì˜ í† í° ê°œìˆ˜ì—ì„œ ì‹œì‘ê³¼ ì¢…ë£Œ í† í°ì˜ ê°œìˆ˜ë¥¼ ëºë‹ˆë‹¤.\n",
    "text_token_count = splitter.count_tokens(\n",
    "    text=text) - count_start_and_stop_tokens\n",
    "print(text_token_count)  # ê³„ì‚°ëœ í…ìŠ¤íŠ¸ í† í° ê°œìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f02ce",
   "metadata": {},
   "source": [
    "### â˜‘ï¸ BERT ê³„ì—´ í† í¬ë‚˜ì´ì €\n",
    "\n",
    "- [CLS] : ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤í˜ì…œ í† í°\n",
    "- [SEP] : ë¬¸ì¥ ëì„ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤í˜ì…œ í† í°\n",
    "\n",
    "ì´ í† í° ìˆ˜ = ë³¸ë¬¸ í† í° ìˆ˜ + 2 ê°€ ë˜ë¯€ë¡œ ì²˜ìŒê³¼ ëì˜ í† í° ìˆ˜ë¥¼ ë¹¼ê³  ê³„ì‚°í•´ì„œ ë³¸ë¬¸ í† í° ê°œìˆ˜ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b6a73",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "- Natural Language Toolkit\n",
    "\n",
    "- ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë¡œê·¸ë¨ ëª¨ìŒ\n",
    "\n",
    "- \\n\\n ë°©ì‹ì´ ì•„ë‹Œ NLTK ë°©ì‹ìœ¼ë¡œ ë¶„í• í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì „ì²˜ë¦¬, í† í°í™”, í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹… ë“± ìˆ˜í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32477376",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "\n",
    "- í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "\n",
    "- ê°ì • ë¶„ì„\n",
    "\n",
    "- ë¬¸ì¥ ë‹¨ìœ„ ì²˜ë¦¬\n",
    "\n",
    "- í’ˆì‚¬ íƒœê¹…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd2cb6",
   "metadata": {},
   "source": [
    "### Hugging Face tokenizer\n",
    "\n",
    "- Huggin Face í† í¬ë‚˜ì´ì €ì— ì˜í•´ ê³„ì‚°ëœ í† í° ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• \n",
    "\n",
    "- ì•„ë˜ì˜ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# GPT-2 ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9293287",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76beecfe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b2529",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ì‹œë©˜í‹± ì²­ì»¤(SemanticChunker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717cc8c8",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ì‹œë©˜í‹± ì²­ì»¤ë€ ?\n",
    "\n",
    "- ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ëŠ” ê²ƒì´ë‹¤.\n",
    "\n",
    "- ë”°ë¼ì„œ ëšëš ëŠê¹€ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ì„ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "- ì„ë² ë”© ëª¨ë¸ì— ì£¼ë¡œ ì“°ì¸ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5121c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfa3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 0.3.23\n",
      "Uninstalling langchain-0.3.23:\n",
      "  Successfully uninstalled langchain-0.3.23\n",
      "Found existing installation: langchain-openai 0.3.13\n",
      "Uninstalling langchain-openai-0.3.13:\n",
      "  Successfully uninstalled langchain-openai-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.52)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.16)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (1.74.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mungi/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: langchain-openai, langchain\n",
      "Successfully installed langchain-0.3.23 langchain-openai-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall langchain langchain-openai -y\n",
    "%pip install --upgrade --no-cache-dir langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73799be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a224c3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bf99f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a6659",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ì½”ë“œ ë¶„í• "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43682991",
   "metadata": {},
   "source": [
    "### ğŸ§‘â€ğŸ’» Split Code\n",
    "\n",
    "- ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ë¶„í• í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fc393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c05f18",
   "metadata": {},
   "source": [
    "### í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë³„ ë¶„í•  ê¸°ì¤€ê°’ì„ í™•ì¸í•˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4456fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (Language, RecursiveCharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8adca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl',\n",
       " 'haskell',\n",
       " 'elixir',\n",
       " 'powershell']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c053122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6c8df",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce4d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='hello_world()')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=50, \n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f071ec",
   "metadata": {},
   "source": [
    "- Document ìƒì„±í•˜ì—¬ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b76958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='hello_world()')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = splitter.create_documents([PYTHON_CODE])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642973f4",
   "metadata": {},
   "source": [
    "- í•„ìš”í•œ ë¶„í•  ë¶€ë¶„ì—ì„œ êµ¬ë¶„ì í‘œì‹œí•˜ì—¬ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f94dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "==================\n",
      "hello_world()\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for doc in result:\n",
    "    print(doc.page_content, end=\"\\n==================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce994f",
   "metadata": {},
   "source": [
    "### JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d7bab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "JS_CODE = \"\"\"\n",
    "function helloWorld() {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, \n",
    "    chunk_size=65, \n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25fbd565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'),\n",
       " Document(metadata={}, page_content='helloWorld();')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = splitter.create_documents([JS_CODE])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "964e03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function helloWorld() {\n",
      "  console.log(\"Hello, World!\");\n",
      "}\n",
      "==================\n",
      "helloWorld();\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for doc in result:\n",
    "    print(doc.page_content, end=\"\\n==================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c50d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7c2a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d580c2d",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ë§ˆí¬ë‹¤ìš´ í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (MarkdownHeaderTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe1660",
   "metadata": {},
   "source": [
    "### â“‚ï¸ ë§ˆí¬ë‹¤ìš´ í—¤ë”ë€?\n",
    "\n",
    "- í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ê°„ë‹¨í•œ ë¬¸ì„œ ì‘ì„± ì–¸ì–´ì´ë‹¤.\n",
    "\n",
    "- í—¤ë”ëŠ” ë¬¸ì„œì˜ ì œëª©ì´ë‚˜ ì„¹ì…˜ì„ êµ¬ë¶„í•œë‹¤.\n",
    "\n",
    "- í—¤ë”ëŠ” # ê¸°í˜¸ë¡œ í‘œí˜„ëœë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd9a28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0696b2",
   "metadata": {},
   "source": [
    "### Markdown ë¬¸ì„œì˜ í—¤ë”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e697e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ì œëª©\n",
      "\n",
      "## 1. ì†Œì œëª©\n",
      "\n",
      "ë‚˜ëŠ” ë¬¸ê¸°ë‹¤.\n",
      "\n",
      "ê·¸ëŠ” ìš©ì´ë‹¤.\n",
      "\n",
      "### 1-1. ë¶€ì œëª© \n",
      "\n",
      "ê·¸ëŠ” í˜„ì´ë‹¤. \n",
      "\n",
      "## 2. ì œëª©\n",
      "\n",
      "ê·¸ëŠ” ë°”ë³´ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "markdown = \"# ì œëª©\\n\\n## 1. ì†Œì œëª©\\n\\në‚˜ëŠ” ë¬¸ê¸°ë‹¤.\\n\\nê·¸ëŠ” ìš©ì´ë‹¤.\\n\\n### 1-1. ë¶€ì œëª© \\n\\nê·¸ëŠ” í˜„ì´ë‹¤. \\n\\n## 2. ì œëª©\\n\\nê·¸ëŠ” ë°”ë³´ë‹¤.\"\n",
    "print(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1e96eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‚˜ëŠ” ë¬¸ê¸°ë‹¤.  \n",
      "ê·¸ëŠ” ìš©ì´ë‹¤.\n",
      "{'Header 1': 'ì œëª©', 'Header 2': '1. ì†Œì œëª©'}\n",
      "=====================\n",
      "ê·¸ëŠ” í˜„ì´ë‹¤.\n",
      "{'Header 1': 'ì œëª©', 'Header 2': '1. ì†Œì œëª©', 'Header 3': '1-1. ë¶€ì œëª©'}\n",
      "=====================\n",
      "ê·¸ëŠ” ë°”ë³´ë‹¤.\n",
      "{'Header 1': 'ì œëª©', 'Header 2': '2. ì œëª©'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "headers_split = [\n",
    "    (\n",
    "        \"#\",\n",
    "        \"Header 1\",\n",
    "    ), \n",
    "    (\n",
    "        \"##\",\n",
    "        \"Header 2\",\n",
    "    ), \n",
    "    (\n",
    "        \"###\",\n",
    "        \"Header 3\",\n",
    "    ), \n",
    "]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_split)\n",
    "\n",
    "result = splitter.split_text(markdown)\n",
    "\n",
    "for header in result:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbfeb1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc8002",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a64e3",
   "metadata": {},
   "source": [
    "## ğŸ“Œ HTML í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (HTMLHeaderTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590a07e",
   "metadata": {},
   "source": [
    "### ğŸ’¾ HTML í—¤ë”ë€?\n",
    "\n",
    "- í—¤ë” íƒœê·¸ : `<h1>, <h2>, <h3>, <h4>`\n",
    "\n",
    "- ì œëª© ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” íƒœê·¸ê°’ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e21a3",
   "metadata": {},
   "source": [
    "### HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "074663dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n",
      "Some intro text about Foo.\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n",
      "Bar main section\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section'}\n",
      "=====================\n",
      "Some intro text about Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section'}\n",
      "=====================\n",
      "Bar subsection 1\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}\n",
      "=====================\n",
      "Some text about the first subtopic of Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}\n",
      "=====================\n",
      "Bar subsection 2\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}\n",
      "=====================\n",
      "Some text about the second subtopic of Bar.\n",
      "{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}\n",
      "=====================\n",
      "Baz\n",
      "{'Header 1': 'Foo', 'Header 2': 'Baz'}\n",
      "=====================\n",
      "Some text about Baz  \n",
      "Some concluding text about Foo\n",
      "{'Header 1': 'Foo'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_split = [\n",
    "    (\"h1\", \"Header 1\"),  # ë¶„í• í•  í—¤ë” íƒœê·¸ì™€ í•´ë‹¹ í—¤ë”ì˜ ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_split)\n",
    "\n",
    "result = splitter.split_text(html_string)\n",
    "\n",
    "for header in result:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b16d6b",
   "metadata": {},
   "source": [
    "### URLì—ì„œ HTML ë¡œë“œ ë° ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c13b7fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Theorem 2 there is a sentence Ï† with the property  \n",
      "y  \n",
      "x  \n",
      "x  \n",
      "y  \n",
      "[ ]  \n",
      "12  \n",
      "âŠ¢ (Ï† â†”\n",
      "Â¬Prov( )).  \n",
      "P  \n",
      "âŒˆ  \n",
      "Ï†  \n",
      "âŒ‰  \n",
      "Thus Ï† says â€˜I am not provable.â€™ We now observe, if âŠ¢ Ï†, then by (1) there is such that âŠ¢ Prf( , ), hence âŠ¢ Prov( ), hence,\n",
      "by (3) âŠ¢ Â¬Ï†, so is inconsistent.\n",
      "Thus  \n",
      "P  \n",
      "n  \n",
      "P  \n",
      "n  \n",
      "âŒˆ  \n",
      "Ï†  \n",
      "âŒ‰  \n",
      "P  \n",
      "âŒˆ  \n",
      "Ï†  \n",
      "âŒ‰  \n",
      "P  \n",
      "P  \n",
      "âŠ¬ Ï†  \n",
      "P  \n",
      "Furthermore, by (4) and (2), we have âŠ¢\n",
      "Â¬Prf( , ) for all natural\n",
      "numbers . By Ï‰-consistency âŠ¬\n",
      "âˆƒ Prf( , ). Thus (3) gives âŠ¬ Â¬Ï†. We have shown that if is\n",
      "{'Header 1': 'Kurt GÃ¶del'}\n",
      "=====================\n",
      "Ï‰-consistent, then Ï† is independent of .  \n",
      "P  \n",
      "n  \n",
      "âŒˆ  \n",
      "Ï†  \n",
      "âŒ‰  \n",
      "n  \n",
      "P  \n",
      "x  \n",
      "x  \n",
      "âŒˆ  \n",
      "Ï†  \n",
      "âŒ‰  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "On concluding the proof of the first theorem, GÃ¶del remarks,\n",
      "â€œwe can readily see that the proof just given is constructive;\n",
      "that is â€¦ proved in an intuitionistically unobjectionable\n",
      "mannerâ€¦â€ (GÃ¶del 1986, p. 177). This is because, as\n",
      "he points out, all the existential statements are based on his theorem\n",
      "V (giving the numeralwise expressibility of primitive recursive\n",
      "{'Header 1': 'Kurt GÃ¶del'}\n",
      "=====================\n",
      "relations), which is intuitionistically unobjectionable.  \n",
      "2.2.3 The Second Incompleteness Theorem  \n",
      "The Second Incompleteness Theorem establishes the unprovability, in\n",
      "number theory, of the consistency of number theory. First we have to\n",
      "write down a number-theoretic formula that expresses the consistency\n",
      "of the axioms. This is surprisingly simple. We just let\n",
      "Con( ) be the sentence Â¬Prov( ).  \n",
      "P  \n",
      "âŒˆ  \n",
      "0 =\n",
      "1  \n",
      "âŒ‰  \n",
      "(GÃ¶delâ€™s Second Incompleteness\n",
      "Theorem) If is consistent, then Con( ) is not\n",
      "{'Header 1': 'Kurt GÃ¶del'}\n",
      "=====================\n",
      "provable from .  \n",
      "Theorem 4  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "Let Ï† be as in (3). The reasoning used to infer\n",
      "â€˜if âŠ¢ Ï†, then âŠ¢ 0 â‰ \n",
      "1â€˜ does not go beyond elementary number theory, and can\n",
      "therefore, albeit with a lot of effort (see below), be formalized in . This yields: âŠ¢\n",
      "(Prov( ) â†’\n",
      "Â¬Con( )), and thus by (3), âŠ¢\n",
      "(Con( ) â†’ Ï†). Since âŠ¬ Ï†, we\n",
      "must have âŠ¬ Con( ).  \n",
      "Proof:  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "âŒˆ  \n",
      "Ï†  \n",
      "âŒ‰  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "The above proof (sketch) of the Second Incompleteness Theorem is\n",
      "{'Header 1': 'Kurt GÃ¶del'}\n",
      "=====================\n",
      "deceptively simple as it avoids the formalization. A rigorous proof\n",
      "would have to establish the proof of â€˜if âŠ¢\n",
      "Ï†, then âŠ¢ 0 â‰  1â€™ in .  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "It is noteworthy that Ï‰-consistency is not needed in the proof\n",
      "of GÃ¶delâ€™s Second Incompleteness Theorem. Also note that\n",
      "neither is Â¬Con( ) provable, by the consistency of and the fact, now known as LÃ¶bâ€™s theorem, that âŠ¢\n",
      "Prov( ) implies âŠ¢ Ï†.  \n",
      "P  \n",
      "P  \n",
      "P  \n",
      "âŒˆ  \n",
      "Ï†  \n",
      "âŒ‰  \n",
      "P  \n",
      "The assumption of Ï‰-consistency in the First Incompleteness\n",
      "{'Header 1': 'Kurt GÃ¶del'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "\n",
    "headers_split = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_split)\n",
    "\n",
    "html_header_splits = splitter.split_text_from_url(url)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "\n",
    "# ë¶„í• ëœ í…ìŠ¤íŠ¸ ì¤‘ 80ë²ˆì§¸ë¶€í„° 85ë²ˆì§¸ê¹Œì§€ì˜ ì²­í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "for header in splits[80:85]:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644760d",
   "metadata": {},
   "source": [
    "- íŠ¹ì • í—¤ë”ë¥¼ ëˆ„ë½ì‹œí‚¤ë©°, êµ¬ì¡°ì  ì°¨ì´ì— ëŒ€í•œ í•œê³„ê°€ ì¡´ì¬í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3242b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403a76c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e5265",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ì¬ê·€ì  JSON ë¶„í• (RecursiveJsonSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff13600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# JSON ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd4c35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63edd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_chunks = splitter.split_json(json_data=json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6dbcf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}\n",
      "=======================================================================================================================================================================================================================================\n",
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}\n"
     ]
    }
   ],
   "source": [
    "# JSON ë°ì´í„°ë¥¼ ë¬¸ì„œ ìƒì„±\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "# JSON ë°ì´í„°ë¥¼ ë¬¸ìì—´ ì²­í¬ ìƒì„±\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë¬¸ìì—´ì„ ì¶œë ¥\n",
    "print(docs[0].page_content)\n",
    "\n",
    "print(\"===\" * 77)\n",
    "\n",
    "# ë¶„í• ëœ ë¬¸ìì—´ ì²­í¬ë¥¼ ì¶œë ¥\n",
    "print(texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
