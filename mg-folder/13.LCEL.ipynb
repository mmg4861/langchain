{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710e5023",
   "metadata": {},
   "source": [
    "# 📙 LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129537a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c472ad",
   "metadata": {},
   "source": [
    "## 📌 RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc4af6",
   "metadata": {},
   "source": [
    "### 🎫 RunnablePassthrough 란?\n",
    "\n",
    "- 입력을 아무 변경 없이 그대로 넘기는 블록 (데이터 전달 역할)\n",
    "\n",
    "- 용도 : 자리 맡기용, 연결용, 디버깅용, 공백 처리용\n",
    "\n",
    "- 일반적으로 `RunnableParallel` 과 함께 활용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfee74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f848cff",
   "metadata": {},
   "source": [
    "## 📌 RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1d865",
   "metadata": {},
   "source": [
    "### 🔃 RunnableParallel 란?\n",
    "\n",
    "- 여러 개의 Runnable을 동시에 실행해주는 블록\n",
    "\n",
    "- 한 개의 입력을 여러 개로 받고, 각각의 대한 결과물을 여러 개의 결과로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cdb14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2436d2",
   "metadata": {},
   "source": [
    "### ✅ 검색기 사용에서 RunnablePassthrough를 사용하는 사례 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ccb8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 문서\n",
    "docs = [\n",
    "    \"문기는 랭체인이 좋습니다.\",\n",
    "    \"민기는 랭체인이 싫습니다.\",\n",
    "    \"문기의 직업은 엔지니어입니다.\",\n",
    "    \"민기의 직업은 gang입니다.\"\n",
    "]\n",
    "vectorstore = FAISS.from_texts(docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 벡터 저장소를 검색기로 사용\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 템플릿 정의\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 템플릿으로부터 채팅 프롬프트를 생성합니다.\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# 문서를 포맷팅하는 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 검색 체인 구성\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490ca9d",
   "metadata": {},
   "source": [
    "### 실행 및 결과 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c87385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문기의 직업은 엔지니어입니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"문기의 직업은 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79db07b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'민기는 gang입니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"민기는 뭐하는 놈이야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8149b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주어진 맥락에는 \"RunnablePassthrough\"에 대한 정보가 포함되어 있지 않습니다. 따라서 답변할 수 없습니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"RunnablePassthrough은 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fae19a",
   "metadata": {},
   "source": [
    "### 정리하기\n",
    "\n",
    "☑️ \"context\" : retriever + format_docs 실행 결과\n",
    "\n",
    "☑️ \"question\" : 입력값 그대로 들어가야 함\n",
    "\n",
    "☑️ RunnablePassthrough() : 질문을 아무 수정 없이 그대로 다음 단계에 전달 목적용\n",
    "\n",
    "☑️ RunnablePassthrough는 필수 인가?\\\n",
    "-> \"question\" 키를 처리하기 위해 필수 사항임❕\\\n",
    "-> 해당 클래스일 필요는 없지만 이를 대신할 무언가는 필수로 존재해야만 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8401c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a9b26",
   "metadata": {},
   "source": [
    "### ✅ 같은 질문에 서로 다른 프롬프트를 병렬로 적용해서 RunnableParallel 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17dc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 프롬프트1 체인 정의\n",
    "capital_chain = (\n",
    "    ChatPromptTemplate.from_template(\"{country} 의 수도는 어디입니까?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 프롬프트2 체인 정의\n",
    "area_chain = (\n",
    "    ChatPromptTemplate.from_template(\"{country} 의 면적은 얼마입니까?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 병렬 실행 용 RunnableParallel 객체 생성\n",
    "map_chain = RunnableParallel(capital=capital_chain, area=area_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03328b2b",
   "metadata": {},
   "source": [
    "### 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d183dc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': '서울입니다.', 'area': '대한민국의 면적은 약 100,210 제곱 킬로미터입니다.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문하기\n",
    "map_chain.invoke({\"country\": \"대한민국\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43efe99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23eec66",
   "metadata": {},
   "source": [
    "### ✅ RunnablePassthrough와 RunnableParallel 함께 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9127b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 문서\n",
    "docs = [\n",
    "    \"대한민국의 기후는 사계절이 뚜렷하다.\",\n",
    "    \"대한민국은 겨울에 눈이 오고 여름엔 장마가 있다.\",\n",
    "    \"대한민국의 수도는 서울이다.\",\n",
    "]\n",
    "vectorstore = FAISS.from_texts(docs, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM & 파서\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# format_docs 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "retriever_chain = retriever | format_docs\n",
    "\n",
    "# 그대로 질문하는 체인 (Passthrough)\n",
    "chain_direct = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | ChatPromptTemplate.from_template(\"질문: {question}\\n정확히 대답해줘.\")\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# RAG 체인 (문서 기반)\n",
    "chain_rag = (\n",
    "    {\"context\": retriever_chain, \"question\": RunnablePassthrough()}\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"다음 문서를 참고하여 질문에 답하세요.\\n문서: {context}\\n질문: {question}\"\n",
    "    )\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# 병렬로 실행 (RunnableParallel)\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"LLM 단독 답변\": chain_direct,\n",
    "    \"LLM & RAG 답변\": chain_rag\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06cf21",
   "metadata": {},
   "source": [
    "### 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1386caf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LLM 단독 답변]\n",
      "\n",
      "대한민국의 기후는 대체로 온대몬순 기후로 분류됩니다. 사계절이 뚜렷하게 나타나는 것이 특징이며, 각 계절의 기온과 강수량에 따라 다양한 날씨 패턴을 보입니다.\n",
      "\n",
      "1. **봄(3월~5월)**: 맑고 따뜻한 날씨가 많으며, 꽃이 만개하는 시기입니다. 평균 기온은 약 10도에서 20도 사이입니다. 그러나 봄철에는 황사와 미세먼지가 발생할 수 있습니다.\n",
      "\n",
      "2. **여름(6월~8월)**: 덥고 습한 날씨가 특징이며, 평균 기온은 25도에서 35도 사이입니다. 이 시기에는 장마가 시작되어 많은 비가 내리며, 습도가 높아져 불쾌지수가 올라갑니다.\n",
      "\n",
      "3. **가을(9월~11월)**: 맑고 선선한 날씨로, 단풍이 아름답게 물드는 시기입니다. 평균 기온은 약 10도에서 20도 사이로, 쾌적한 날씨가 지속됩니다.\n",
      "\n",
      "4. **겨울(12월~2월)**: 춥고 건조한 날씨가 이어지며, 평균 기온은 0도에서 -10도 사이입니다. 특히, 북부지역은 눈이 많이 오고, 남부지역은 상대적으로 온난한 겨울을 경험합니다.\n",
      "\n",
      "전반적으로 대한민국은 사계절의 변화가 뚜렷하여 각 계절마다 다양한 자연 경관과 기후 특성을 즐길 수 있습니다. 또한, 지역에 따라 기후 차이가 있어, 서울과 부산, 제주도 등에서의 기온과 강수량은 다르게 나타날 수 있습니다.\n",
      "\n",
      "[LLM & RAG 답변]\n",
      "\n",
      "대한민국의 기후는 사계절이 뚜렷합니다. 겨울에는 눈이 내리고, 여름에는 장마가 있습니다. 이러한 특징 덕분에 각 계절마다 다양한 날씨와 풍경을 경험할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "result = parallel_chain.invoke(\"대한민국의 기후가 궁금해\")\n",
    "\n",
    "print(\"\\n[LLM 단독 답변]\\n\")\n",
    "print(result[\"LLM 단독 답변\"])\n",
    "\n",
    "print(\"\\n[LLM & RAG 답변]\\n\")\n",
    "print(result[\"LLM & RAG 답변\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208486a3",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7ecc7",
   "metadata": {},
   "source": [
    "## 📌 Runnable 구조(그래프) 검토"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ecde4",
   "metadata": {},
   "source": [
    "### 📈 Runnable 구조(그래프) 검토\n",
    "- Runnable의 흐름을 이해하기 위한 그래프 그려보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0a398",
   "metadata": {},
   "source": [
    "### 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 설치\n",
    "!pip install -qU faiss-cpu tiktoken\n",
    "\n",
    "# 그래프 라이브러리 설치\n",
    "!pip install -qU grandalf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7258bf",
   "metadata": {},
   "source": [
    "### 일반적인 체인 구성 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e903c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    # 텍스트 데이터로부터 FAISS 벡터 저장소를 생성합니다.\n",
    "    [\"Teddy는 AI 엔지니어입니다.\", \"Teddy는 프로그래밍을 좋아합니다!\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# 벡터 저장소를 기반으로 retriever를 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"다음 맥락만을 토대로 질문에 답하세요.:\n",
    "{context}  \n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "# 템플릿을 기반으로 ChatPromptTemplate을 생성합니다.\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template\n",
    ")\n",
    "\n",
    "# ChatOpenAI 모델을 초기화합니다.\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# chain 을 생성합니다.\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39fbcb",
   "metadata": {},
   "source": [
    "### Runnable의 그래프 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10435fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9e99e544dd5f46d39b9827e4f250790b': Node(id='9e99e544dd5f46d39b9827e4f250790b', name='Parallel<context,question>Input', data=<class 'langchain_core.runnables.base.RunnableParallel<context,question>Input'>, metadata=None),\n",
       " '1961b2b62b8d496f86fb81ffb2d0029e': Node(id='1961b2b62b8d496f86fb81ffb2d0029e', name='Parallel<context,question>Output', data=<class 'langchain_core.utils.pydantic.RunnableParallel<context,question>Output'>, metadata=None),\n",
       " '2b3e089446224bdda73e584caed61770': Node(id='2b3e089446224bdda73e584caed61770', name='VectorStoreRetriever', data=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1694c8690>, search_kwargs={}), metadata=None),\n",
       " 'dd95ff52bf2947749987f4c8ad52350e': Node(id='dd95ff52bf2947749987f4c8ad52350e', name='Passthrough', data=RunnablePassthrough(), metadata=None),\n",
       " '9f1ac3dec1f74dcd9a7a92d1dd28ffe5': Node(id='9f1ac3dec1f74dcd9a7a92d1dd28ffe5', name='ChatPromptTemplate', data=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}  \\n\\nQuestion: {question}'), additional_kwargs={})]), metadata=None),\n",
       " '9b7f66f84a6f4546bbf919bcc480f5dc': Node(id='9b7f66f84a6f4546bbf919bcc480f5dc', name='ChatOpenAI', data=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x168ccc150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x168db4bd0>, root_client=<openai.OpenAI object at 0x168d4b2d0>, root_async_client=<openai.AsyncOpenAI object at 0x16973e1d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), metadata=None),\n",
       " 'a1bf6c53db194cd5b4d51ac97e031929': Node(id='a1bf6c53db194cd5b4d51ac97e031929', name='StrOutputParser', data=StrOutputParser(), metadata=None),\n",
       " 'c07c81c584434144836022291a8d11dd': Node(id='c07c81c584434144836022291a8d11dd', name='StrOutputParserOutput', data=<class 'langchain_core.output_parsers.string.StrOutputParserOutput'>, metadata=None)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 체인의 그래프에서 노드 가져오기\n",
    "chain.get_graph().nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc52cc",
   "metadata": {},
   "source": [
    "- 노드(Node): 각각의 처리 단계 (예: retriever, prompt, model 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74379298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Edge(source='c525746f8cf2494eb90f2c08097eded9', target='5e45775b2cb444c0a674d3d5004a1760', data=None, conditional=False),\n",
       " Edge(source='5e45775b2cb444c0a674d3d5004a1760', target='863efcd269944b3486ace0c82316e1f1', data=None, conditional=False),\n",
       " Edge(source='c525746f8cf2494eb90f2c08097eded9', target='d50f4826cfc74a28bdc8e3aaa3e31292', data=None, conditional=False),\n",
       " Edge(source='d50f4826cfc74a28bdc8e3aaa3e31292', target='863efcd269944b3486ace0c82316e1f1', data=None, conditional=False),\n",
       " Edge(source='863efcd269944b3486ace0c82316e1f1', target='46cae50d6a4e4934a1440d4afc4be280', data=None, conditional=False),\n",
       " Edge(source='46cae50d6a4e4934a1440d4afc4be280', target='f667874687d14d7fbe43aa9ffe4812da', data=None, conditional=False),\n",
       " Edge(source='ed2d1a69a5634b40b8f5e711d7004b53', target='6270c2309d4b4d4390a2cbc297860425', data=None, conditional=False),\n",
       " Edge(source='f667874687d14d7fbe43aa9ffe4812da', target='ed2d1a69a5634b40b8f5e711d7004b53', data=None, conditional=False)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 체인의 그래프에서 엣지를 가져오기\n",
    "chain.get_graph().edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cccba",
   "metadata": {},
   "source": [
    "- 엣지(Edge): 그 처리 단계들 사이를 연결하는 “화살표”나 “선”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c9a5c",
   "metadata": {},
   "source": [
    "### 그래프 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868de14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           +---------------------------------+         \n",
      "           | Parallel<context,question>Input |         \n",
      "           +---------------------------------+         \n",
      "                    **               **                \n",
      "                 ***                   ***             \n",
      "               **                         **           \n",
      "+----------------------+              +-------------+  \n",
      "| VectorStoreRetriever |              | Passthrough |  \n",
      "+----------------------+              +-------------+  \n",
      "                    **               **                \n",
      "                      ***         ***                  \n",
      "                         **     **                     \n",
      "           +----------------------------------+        \n",
      "           | Parallel<context,question>Output |        \n",
      "           +----------------------------------+        \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                  +--------------------+               \n",
      "                  | ChatPromptTemplate |               \n",
      "                  +--------------------+               \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                      +------------+                   \n",
      "                      | ChatOpenAI |                   \n",
      "                      +------------+                   \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                   +-----------------+                 \n",
      "                   | StrOutputParser |                 \n",
      "                   +-----------------+                 \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                +-----------------------+              \n",
      "                | StrOutputParserOutput |              \n",
      "                +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "# 체인의 그래프를 ASCII 형식으로 출력\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b2601",
   "metadata": {},
   "source": [
    "### ☑️ 정리하기\n",
    "\n",
    "1.\tParallelInput: 체인의 맨 앞에서 \"context\"와 \"question\" 두 개의 key에 대해 동시에 처리 준비\n",
    "\n",
    "2.\tVectorStoreRetriever: \"context\" 값 생성을 위해 retriever 실행 (벡터 검색)\n",
    "\n",
    "3.\tPassthrough: \"question\"은 입력값 그대로 통과\n",
    "\n",
    "4.\tParallelOutput: \"context\" + \"question\" → 딕셔너리로 묶어서 다음 단계로 넘김\n",
    "\n",
    "5.\tChatPromptTemplate: {context}\\nQuestion: {question} 형태의 프롬프트 생성\n",
    "\n",
    "6.\tChatOpenAI: 프롬프트를 LLM(GPT-4o-mini)에 전달 → 응답 생성\n",
    "\n",
    "7.\tStrOutputParser: 모델 응답을 파싱해서 문자열로 변환\n",
    "\n",
    "8.\tStrOutputParserOutput: 최종 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da67315",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d704f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6cac7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f194c1e0",
   "metadata": {},
   "source": [
    "## 📌 동적 속성 지정 (configurable_fields, configurable_alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df6317",
   "metadata": {},
   "source": [
    "### ⛓️‍💥 런타임에 체인 내부 구성하기\n",
    "\n",
    "- `configurable_fields` 시스템의 설정 값을 정의하는 필드를 의미합니다.\n",
    "\n",
    "- `configurable_alternatives` 런타임에 설정할 수 있는 Runnable 대안을 구성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa78b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681551f3",
   "metadata": {},
   "source": [
    "### configurable_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07a6b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default=ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}'), additional_kwargs={})]) fields={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "print(prompt.configurable_fields())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3dfbf",
   "metadata": {},
   "source": [
    "-> configurable_fields의 의해서 해당 템플릿은 교체 가능한 필드로 지정됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a556d",
   "metadata": {},
   "source": [
    "### 언제 쓰이는가 ⁉️\n",
    "\n",
    "- 런타임에 프롬프트를 교체하는 경우\n",
    "\n",
    "- 동적으로 모델을 변경하거나, 파라미터 값을 실험하는 경우\n",
    "\n",
    "- 설정 값을 .with_config() 등으로 바꿀 경우\n",
    "\n",
    "- 서버, UI 등에서 외부 설정 주입해야하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294aa2c",
   "metadata": {},
   "source": [
    "### 런타임에 프롬프트를 교체하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a39d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 기본 프롬프트\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# 기본 체인 구성\n",
    "chain = prompt | ChatOpenAI()\n",
    "\n",
    "# 프롬프트를 교체하고 싶다면?\n",
    "new_prompt = ChatPromptTemplate.from_template(\"Explain {topic} to a 5-year-old\")\n",
    "chain = chain.with_config(configurable={\"prompt\": new_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0490bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default=RunnableBinding(bound=ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}'), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12215d550>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x120c5a150>, root_client=<openai.OpenAI object at 0x121287210>, root_async_client=<openai.AsyncOpenAI object at 0x1224c7d90>, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={}, config={'configurable': {'prompt': ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain {topic} to a 5-year-old'), additional_kwargs={})])}}, config_factories=[]) fields={}\n"
     ]
    }
   ],
   "source": [
    "print(chain.configurable_fields())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e53ef6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541cbe6",
   "metadata": {},
   "source": [
    "### 동적으로 모델을 변경하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afdcd74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '대한민국의 수도는 서울입니다.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 8,\n",
       "   'prompt_tokens': 15,\n",
       "   'total_tokens': 23,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-2024-08-06',\n",
       "  'system_fingerprint': 'fp_a288987b44',\n",
       "  'id': 'chatcmpl-Bp6z13FM6gROnpOXHaNnflloxxKIt',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-bff8624b-0038-4fa1-93da-5fd01f367787-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 15,\n",
       "  'output_tokens': 8,\n",
       "  'total_tokens': 23,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "model.invoke(\"대한민국의 수도는 어디야?\").__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b595e",
   "metadata": {},
   "source": [
    "`configurable_fields` 메서드를 사용하여 `model_name` 속성을 동적 구성 가능한 필드로 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "868c7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0).configurable_fields(\n",
    "    model_name=ConfigurableField(  # model_name 은 원래 ChatOpenAI 의 필드입니다.\n",
    "        id=\"gpt_version\",  # model_name 의 id 를 설정합니다.\n",
    "        name=\"Version of GPT\",  # model_name 의 이름을 설정합니다.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a8250",
   "metadata": {},
   "source": [
    "`model.invoke()` 호출시 `config={\"configurable\": {\"키\": \"값\"}}` 형식으로 동적 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4780eb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '대한민국의 수도는 서울이다.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 16,\n",
       "   'prompt_tokens': 22,\n",
       "   'total_tokens': 38,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-3.5-turbo-0125',\n",
       "  'system_fingerprint': None,\n",
       "  'id': 'chatcmpl-Bp7ztHPzsqMM7XDZSXRJderAhtTz7',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-4f664bb5-2f59-4d5e-ab58-dbd682f7729f-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 22,\n",
       "  'output_tokens': 16,\n",
       "  'total_tokens': 38,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    \"대한민국의 수도는 어디야?\",\n",
    "    # gpt_version 을 gpt-3.5-turbo 로 설정합니다.\n",
    "    config={\"configurable\": {\"gpt_version\": \"gpt-3.5-turbo\"}},\n",
    ").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64cfdcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '대한민국의 수도는 서울입니다.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 8,\n",
       "   'prompt_tokens': 15,\n",
       "   'total_tokens': 23,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "  'system_fingerprint': 'fp_34a54ae93c',\n",
       "  'id': 'chatcmpl-Bp74KPMOKmYd5dmx5ySfMnRLKoBa4',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-d7bb556f-53d2-4705-a62e-a03716affe25-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 15,\n",
       "  'output_tokens': 8,\n",
       "  'total_tokens': 23,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    # gpt_version 을 gpt-4o-mini 로 설정합니다.\n",
    "    \"대한민국의 수도는 어디야?\",\n",
    "    config={\"configurable\": {\"gpt_version\": \"gpt-4o-mini\"}},\n",
    ").__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cacbd4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243248fc",
   "metadata": {},
   "source": [
    "### 설정값을 .with_config() 바꿀 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "126664ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Why did the banana go to the doctor?\\n\\nBecause it wasn't peeling well! 🍌\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 13, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bp8skav460rqF9euxnHAWKwziCYCn', 'finish_reason': 'stop', 'logprobs': None} id='run-145718f4-6063-415e-9ce4-6066b84afd81-0' usage_metadata={'input_tokens': 13, 'output_tokens': 17, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 원래 체인\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "chain = prompt | llm\n",
    "\n",
    "# 프롬프트를 더 친절하게 바꾸고 싶을 때\n",
    "new_prompt = ChatPromptTemplate.from_template(\"Can you kindly tell a joke about {topic}?\")\n",
    "\n",
    "# 프롬프트만 교체\n",
    "reconfigured_chain = chain.with_config(configurable={\"prompt\": new_prompt})\n",
    "\n",
    "# 실행\n",
    "print(reconfigured_chain.invoke({\"topic\": \"bananas\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bfa26",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3be917",
   "metadata": {},
   "source": [
    "### Configuable Alternatives: Runnable 객체 자체의 대안 설정\n",
    "\n",
    "- `ChatAnthropic` 의 구성 가능한 언어 모델은 다양한 작업과 컨텍스트에 적용할 수 있는 유연성을 제공합니다.\n",
    "\n",
    "- 동적으로 설정(Config) 값을 변경하기 위하여 모델에 설정하는 파라미터를 ConfigurableField 객체로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0de27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    temperature=0, model=\"claude-3-5-sonnet-20240620\"\n",
    ").configurable_alternatives(\n",
    "\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "\n",
    "    default_key=\"anthropic\",\n",
    "\n",
    "    openai=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    gpt4o=ChatOpenAI(model=\"gpt-4o\"),\n",
    "\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 간단히 설명해주세요.\")\n",
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic을 기본으로 호출합니다.\n",
    "chain.invoke({\"topic\": \"뉴진스\"}).__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a731e9",
   "metadata": {},
   "source": [
    "{'content': '뉴진스(NewJeans)는 2022년 7월 22일에 데뷔한 대한민국의 5인조 걸그룹입니다. 주요 특징은 다음과 같습니다:\\n\\n1. 소속사: ADOR (HYBE Labels의 자회사)\\n\\n2. 멤버: 민지, 하니, 다니엘, 해린, 혜인\\n\\n3. 데뷔곡: \"Attention\", \"Hype Boy\", \"Cookie\"\\n\\n4. 특징:\\n   - 10대 멤버들로 구성된 신선한 이미지\\n   - Y2K, 90년대 감성을 현대적으로 재해석한 음악과 스타일\\n   - 독특한 마케팅 전략 (사전 홍보 없이 갑작스러운 데뷔)\\n\\n5. 주요 성과:\\n   - 데뷔 앨범 \\'뉴 진스(New Jeans)\\'로 빌보드 200 차트 진입\\n   - 여러 음원 차트 1위 기록\\n   - 각종 신인상 수상\\n\\n뉴진스는 데뷔 이후 빠르게 인기를 얻으며 4세대 K-pop을 대표하는 그룹으로 자리매김하고 있습니다.', 'additional_kwargs': {}, 'response_metadata': {'id': 'msg_01PJfWdBpgp2FbWp1t3HfQP4',  'model': 'claude-3-5-sonnet-20240620',  'stop_reason': 'end_turn',  'stop_sequence': None,  'usage': {'input_tokens': 30, 'output_tokens': 390}}, 'type': 'ai', 'name': None, 'id': 'run-68b3570d-a0d4-4074-9d69-cbf40b3caf8b-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 30,  'output_tokens': 390,  'total_tokens': 420}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b67217",
   "metadata": {},
   "source": [
    "-> ChatAnthropic 모델을 사용하기 위하여 API KEY를 발급받아 설정해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4f650",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4274a96",
   "metadata": {},
   "source": [
    "## 📌 @chain 데코레이터를 사용하여 Runnable 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12971d",
   "metadata": {},
   "source": [
    "- `@chain` 데코레이터를 이용해 파이썬 함수를 Runnable 객체처럼 사용할 수 있게 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a5b09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326dfcd",
   "metadata": {},
   "source": [
    "ChatPromptTemplate 클래스를 사용하여 두 개의 프롬프트 템플릿을 정의합니다.\n",
    "\n",
    "prompt1 은 주어진 주제에 대한 짧은 설명을, prompt2 는 영어로 번역해 달라는 요청 프롬프트 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20e172ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿을 정의합니다.\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{topic} 에 대해 짧게 한글로 설명해주세요.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{sentence} 를 emoji를 활용한 인스타그램 게시글로 만들어주세요.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f34d0",
   "metadata": {},
   "source": [
    "custom_chain 함수는 입력 텍스트를 기반으로 사용자 정의 체인을 실행합니다.\n",
    "\n",
    "@chain 데코레이터로 사용자 정의 함수를 데코레이팅 하며, 데코레이팅을 통해 함수를 Runnable 한 객체로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6a36e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def custom_chain(text):\n",
    "    # 첫 번째 프롬프트, ChatOpenAI, 문자열 출력 파서를 연결하여 체인을 생성합니다.\n",
    "    chain1 = prompt1 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    output1 = chain1.invoke({\"topic\": text})\n",
    "\n",
    "    # 두 번째 프롬프트, ChatOpenAI, 문자열 출력 파서를 연결하여 체인을 생성합니다.\n",
    "    chain2 = prompt2 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    # 두 번째 체인을 호출하여 파싱된 첫 번째 결과를 전달하고 최종 결과를 반환합니다.\n",
    "    return chain2.invoke({\"sentence\": output1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d2980",
   "metadata": {},
   "source": [
    "`custom_chain`은 이제 실행 가능한 객체(runnable)이므로, invoke() 를 사용하여 실행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffad420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨🔬 양자역학에 대해 알아보자! 🔬✨\n",
      "\n",
      "양자역학은 미시 세계, 즉 원자와 아원자 입자의 행동을 설명하는 💫 물리학의 한 분야입니다. \n",
      "\n",
      "고전역학과는 다르게, 양자역학에서는 입자가 동시에 여러 상태에 있을 수 있어요! 😲 또한, 위치와 운동량을 동시에 정확하게 측정할 수 없다는 우주 법칙도 있어요. 🚀🔍\n",
      "\n",
      "✅ 주요 개념:\n",
      "- 파동-입자 이중성 🌊⚛️\n",
      "- 불확정성 원리 ❓🔐\n",
      "- 양자 얽힘 🔗✨\n",
      "\n",
      "양자역학은 현대 물리학과 기술에서 중요한 기초를 제공해요! 💻📡 예를 들어, 반도체, 레이저, 양자 컴퓨팅 등 다양한 분야에서 활용되고 있답니다! 💡\n",
      "\n",
      "함께 신비로운 미시 세계로 떠나볼까요? 🌌🔭 \n",
      "\n",
      "#양자역학 #물리학 #과학사랑 #미시세계 #반도체 #양자컴퓨팅 #레이저\n"
     ]
    }
   ],
   "source": [
    "# custom_chain을 호출\n",
    "print(custom_chain.invoke(\"양자역학\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd987cc1",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d780f74",
   "metadata": {},
   "source": [
    "## 📌 RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1f04d",
   "metadata": {},
   "source": [
    "### 📝 RunnableWithMessageHistory란?\n",
    "\n",
    "- RunnableWithMessageHistory는 LangChain 체인에 “채팅 메시지 히스토리”를 연결해주는 래퍼\n",
    "\n",
    "- 사용자의 대화를 계속 기억하면서 “대화형 챗봇”처럼 동작하게 만들고 싶을 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a067a1",
   "metadata": {},
   "source": [
    "### 실제 활용 예시\n",
    "\n",
    "- 대화형 챗봇 개발: 사용자와의 대화 내역을 기반으로 챗봇의 응답을 조정할 수 있습니다.\n",
    "\n",
    "- 복잡한 데이터 처리: 데이터 처리 과정에서 이전 단계의 결과를 참조하여 다음 단계의 로직을 결정할 수 있습니다.\n",
    "\n",
    "- 상태 관리가 필요한 애플리케이션: 사용자의 이전 선택을 기억하고 그에 따라 다음 화면이나 정보를 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4e652",
   "metadata": {},
   "source": [
    "### 휘발성 대화기록: 인메모리(In-Memory)\n",
    "\n",
    "사용자와 주고받은 메시지를 메모리에만 잠깐 저장하고,\n",
    "\n",
    "프로그램이 종료되면 기억이 사라지는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "308cf388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[human] 안녕?\n",
      "[ai] 안녕하세요! 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# 대화 히스토리 객체 생성 (휘발성)\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "# 사용자 → 시스템 메시지 저장\n",
    "history.add_user_message(\"안녕?\")\n",
    "history.add_ai_message(\"안녕하세요! 무엇을 도와드릴까요?\")\n",
    "\n",
    "# 출력해보기\n",
    "for msg in history.messages:\n",
    "    print(f\"[{msg.type}] {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c0d61",
   "metadata": {},
   "source": [
    "### 휘발성 실사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# 프롬프트 구성\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 체인\n",
    "chain = prompt | ChatOpenAI(model_name=\"gpt-4o-mini\") | StrOutputParser()\n",
    "\n",
    "# 세션별 메모리 저장소\n",
    "store = {}\n",
    "\n",
    "# get_session_history 함수 수정\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 메시지 히스토리를 붙인 체인 구성\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d460a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"user-1234\"\n",
    "\n",
    "print(chat_chain.invoke(\n",
    "    {\"input\": \"한국의 수도는 어디야?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    "))\n",
    "\n",
    "print(chat_chain.invoke(\n",
    "    {\"input\": \"그 도시의 날씨는 어때?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198541d",
   "metadata": {},
   "source": [
    "-> 출력마다 InMemory에 의해 대화 기록이 휘발되어 그 도시라고 질문하게 되면 출력하지 못하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de6207",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bf276",
   "metadata": {},
   "source": [
    "### ✅ 메시지 기록(메모리) 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f4734",
   "metadata": {},
   "source": [
    "### 🛑 RedisChatMessageHistory\n",
    "\n",
    "- Redis를 통한 대화 기록 영구 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91ed9dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7599bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ✅ Redis 세션 히스토리 불러오기 함수\n",
    "def get_history(session_id: str):\n",
    "    return RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=\"redis://223.130.143.231:6379\",  # 🔧 본인 환경에 맞게 조정 (cloud이면 URL로)\n",
    "        ttl=None  # None이면 무제한 저장\n",
    "    )\n",
    "\n",
    "# ✅ 프롬프트 구성 — 반드시 {history} 포함!\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# ✅ 체인 정의\n",
    "base_chain = prompt | ChatOpenAI(model_name=\"gpt-4o-mini\") | StrOutputParser()\n",
    "\n",
    "# ✅ 메시지 히스토리 적용\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a9a6b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국의 수도는 서울입니다.\n",
      "현재 날씨 정보를 실시간으로 제공할 수는 없지만, 서울의 날씨는 계절에 따라 다릅니다. 여름은 덥고 습하고, 겨울은 춥고 건조합니다. 봄과 가을은 비교적 온화하고 쾌적한 날씨가 지속됩니다. 현재 서울의 날씨를 알고 싶다면 기상청 웹사이트나 날씨 앱을 확인하는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "session_id = \"user-5678\"  # 고정된 사용자 ID\n",
    "\n",
    "# 1. 첫 번째 질문\n",
    "print(chat_chain.invoke(\n",
    "    {\"input\": \"한국의 수도는 어디야?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    "))\n",
    "\n",
    "# 2. 이어지는 문맥 질문 (다시 실행해도 Redis가 기억함!)\n",
    "print(chat_chain.invoke(\n",
    "    {\"input\": \"그 도시의 날씨는 어때?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235ed9c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdde38e",
   "metadata": {},
   "source": [
    "### 🛑 RunnableWithMessageHistory\n",
    "\n",
    "- 애플리케이션의 상태를 유지하고, 사용자 경험을 향상시키며, 더 정교한 응답 메커니즘을 구현할 수 있게 해주는 강력한 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ea5ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}  # 세션 기록을 저장할 딕셔너리\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids: str) -> BaseChatMessageHistory:\n",
    "    print(session_ids)\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "with_message_history = (\n",
    "    RunnableWithMessageHistory(  # RunnableWithMessageHistory 객체 생성\n",
    "        runnable,  # 실행할 Runnable 객체\n",
    "        get_session_history,  # 세션 기록을 가져오는 함수\n",
    "        input_messages_key=\"input\",  # 입력 메시지의 키\n",
    "        history_messages_key=\"history\",  # 기록 메시지의 키\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bcfd199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 RunnableWithMessageHistory만 쓰기\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=lambda session_id: ...,  # history return\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "701838cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': 'Ratio of the adjacent side length to the hypotenuse length in a right triangle.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 17,\n",
       "   'prompt_tokens': 160,\n",
       "   'total_tokens': 177,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-3.5-turbo-0125',\n",
       "  'system_fingerprint': None,\n",
       "  'id': 'chatcmpl-BpBmf29kO2x9yMy6ItNq3ClkztIeC',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-9e34f702-7d71-4a5a-b960-aa7aa57af9d7-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 160,\n",
       "  'output_tokens': 17,\n",
       "  'total_tokens': 177,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # 수학 관련 질문 \"코사인의 의미는 무엇인가요?\"를 입력으로 전달합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    # 설정 정보로 세션 ID \"abc123\"을 전달합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5025bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': '직각 삼각형에서 인접한 변의 길이를 빗변의 길이로 나눈 비율입니다.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 41,\n",
       "   'prompt_tokens': 202,\n",
       "   'total_tokens': 243,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-3.5-turbo-0125',\n",
       "  'system_fingerprint': None,\n",
       "  'id': 'chatcmpl-BpBmmVi7wssMOWjPhIrVOFDthCK2e',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-bb95f1e5-d58b-4196-a5e5-008b11f03342-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 202,\n",
       "  'output_tokens': 41,\n",
       "  'total_tokens': 243,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메시지 기록을 포함하여 호출합니다.\n",
    "with_message_history.invoke(\n",
    "    # 능력과 입력을 설정합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"이전의 내용을 한글로 답변해 주세요.\"},\n",
    "    # 설정 옵션을 지정합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ").__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg-langchain-uZ1tu061-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
